max_epochs: 10
model_type: custom_deberta # bert, deberta, custom_deberta, or bert_vat
base_init: [false]

data: lr rl
limit_data: 1.0
splits: 3
seeds: [25, 26, 7]
tag: # optional: name this experiment version
metric: asp_f1
baseline_str: deberta_rnd_init
baseline_version: version_baseline

### Hyperparameters ###
# VAT
gamma: 0.0001
lr_adv: 100
keep_original: False # train with unperturbed examples + perturbed examples

# DeBERTa with Pivot Phrase Embeddings
pivot_phrase_embeddings: True  # must be true for c2p_pp/p2c_pp and c2m/m2c
negative_embeddings: False     # new embeddings are negated copy of rel_embeddings
perturbed_embeddings: False    # new embeddings are copy of rel_embeddings slightly perturbed (randomly)
c2p_att_pp: False              # enable/disable c2p_att_pp component
p2c_att_pp: False              # enable/disable p2c_att_pp component

# DeBERTa with marked attention components
mark_embeddings: random                  # random or binary
mark_projection_initialization: content  # random, content, or position
c2m: True
m2c: True
c2m_scalar: 1
m2c_scalar: 1
########################

num_workers: 44
gpus: [0, 1]
parallel: True

do_train: true
do_predict: true

overwrite_cache: true # if false, loads features from cached file (faster)
cache_dir: # default: nlp-architect/nlp_architect/cache
output_dir: # default: nlp-architect/nlp_architect/cache/li-bert-models
labels: labels.txt # relative to nlp_architect/models/li_bert/data

# Comment depending on model choice
#model_name_or_path: bert-base-uncased
model_name_or_path: microsoft/deberta-base
train_batch_size: 8
eval_batch_size: 8
max_seq_length: 64
adam_epsilon: 1.0e-08
fast_dev_run: false
accumulate_grad_batches: 1
learning_rate: 5.0e-05
gradient_clip_val: 1.0
n_tpu_cores: 0
resume_from_checkpoint: null
tokenizer_name: null
val_check_interval: 1.0
warmup_steps: 0
weight_decay: 0.0
logger: true
